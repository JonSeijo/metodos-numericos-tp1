\section{Resultados finales y experimentación}
\subsection{Resultados}

{\centering
    \includegraphics[scale=0.6]{informe/imagenes/supnivel/supNivelCaballoLucesPropias578N1.pdf} \\
}
{\centering
    \includegraphics[scale=0.6]{informe/imagenes/supnivel/supNivelCaballoLucesPropias578N2.pdf} \\
}
{\centering
    \includegraphics[scale=0.6]{informe/imagenes/supnivel/supNivelCaballoLucesPropias578N3.pdf}
    \captionof{figure}{\textit{Curvas de nivel de la función de profundidad (para cada píxel, su profundidad estimada) Los valores fueron calculados utilizando las luces que obtuvimos en nuestra calibración.}}
}

\newpage
Si bien esperábamos obtener una superficie mucho mas \textit{suave}, creemos que los resultados fueron satisfactorios. Puede observarse perfectamente en los gráficos de curvas de nivel de arriba que en cada píxel la profundidad estimada es adecuada y se corresponde con la iluminación de las fotos originales. Veamos una representación de la función de profundidad con los mismos datos en un modelado 3D:

{\centering
    \includegraphics[scale=0.8]{informe/imagenes/profundidades/profundidadesCaballoLucesPropias578.pdf} \\
    \captionof{figure}{Profundidades calculadas con luces propias, 5, 7, 8}
}

$ $\newline
En la imagen de arriba podemos ver lo que mencionamos sobre la \textit{suavidad} de la superficie. Es claro que es diferente a lo visto en la imagen de ejemplo del enunciado, pero incluso así son distinguibles las alturas de los diferentes píxeles. El \textit{fondo} del modelo es un plano perfecto por el hecho de haber utilizado la máscara para no realizar cálculos innecesarios. Sospechamos que los picos tiene que ver con la forma en que aproximamos los planos tangentes.\\

Veamos que obtenemos si partimos directamente de las \textbf{normales} provistas por la cátedra:

{\centering
    \includegraphics[scale=0.7]{informe/imagenes/profundidadesCaballoNormalescatedra.pdf} \\
}

Si bien las luces con las que fueron obtenidas las normales de la cátedra son desconocidas para nosotros, podemos hacer algunas comparaciones. Aunque esté llena de \textit{picos}, sobretodo en los bordes, parece ser una superficie un poco mas suave que la obtenida por nosotros. Dado que no sabemos cuales fueron las luces utilizadas en el cálculo, no podemos descartar que sea un tema de elección de luces. Sin embargo, incluso aunque fuesen las mismas, veremos mas adelante que lo calibrado por nosotros no se corresponde en un 100\% por lo que es esperable que se observen diferencias. \\

Otra cosa a notar, es que en ambas los detalles del rostro del caballo se pierden. Si bien era esperable ya que son áreas delicadas, nos dá la pauta de que nuestras aproximaciones pueden no ser del todo correctas. De todos modos, no podemos saber si en una estimación 'bien hecha' los detalles se mantienen o no. \\

Nos gustaría ver qué sucede cuando utilizamos otras luces sobre la misma figura y así observar, por ejemplo, si obtenemos la misma cantidad de picos. Además, es importante notar que para obtener las intensidades de iluminosidad estábamos utilizando el promedio de los colores, asi que aprovecharemos la siguiente prueba para ver qué diferencias podemos encontrar si consideramos alguna componente en particular. No esperamos obtener diferencias significativas entre tomar el promedio o una sola componente del color.



{\centering
    \includegraphics[scale=0.5]{informe/imagenes/profundidades/profCaballo012ColorProm.pdf}
    \captionof{figure}{$\uparrow$ Luces 0,1,2, promedio de colores}
    \includegraphics[scale=0.49]{informe/imagenes/profundidades/profCaballo012ColorAzul.pdf}
    % \captionof{figure}{\textit{Luces 0,1,2, componente azul}}
    \includegraphics[scale=0.54]{informe/imagenes/profundidades/profCaballo012ColorRojo.pdf}
    % \captionof{figure}{\textit{Luces 0,1,2, componente roja}}
}
\begin{center}
    Luces 0,1,2, componente azul (izq), componente roja (der)
\end{center}

Cómo sospechabamos, no hay ninguna diferencia apreciable en tomar las imágenes con diferentes componentes de color. Pensamos que una posibilidad es que la nula diferencia se deba al color del objeto original. \\

Es interesante notar que utilizando estas combinaciones de luces obtenemos diferentes resultados que la usando la primer combinación. En la primera habia muchos picos y la diferencia entre blancos y negros era muy pronunciada. Con esta nueva combinación, los colores visible son mucho mas uniformes y la cantidad de picos disminuyó, aunque siguen manteniéndose los picos en los bordes. Siguen sin verse correctamente los detalles en el rostro del caballo. \\

Realizamos los cálculos de profundidad con una imagen que tiene más detalles, para ver si sucede lo mismo que con el rostro del caballo. La siguiente es un gráfico con las profundidades de Buda, con las luces $0, 1$ y $2$ calibradas por nosotros.

{\centering
    \includegraphics[scale=0.8]{informe/imagenes/profundidades/profundidadesBudaLucesPropias012.pdf} \\
}

Los detalles del cuerpo se mantuvieron bastante bien, y el área central e inferior tiene mas suavidad que la que habíamos logrado con la anterior imagen. En los bordes nuevamente tenemos picos, se deben a la diferencia brusca que hay con el plano. Es interesante notar que se produjo una deformación en el sector superior, creemos que tiene que ver con la gran cantidad de cambios de luz en un área tan pequeña. \\


% 1. Comparar las direcciones de iluminacion obtenidas por el metodo de calibracion con las provistas por la catedra \\
\newpage
\subsection{Calibración de luces}

Mencionamos anteriormente que se encontraron diferencias entre las luces calibradas por nosotros y las luces  de la cátedra. Veamos los resultados que obtuvimos. En general, niguna luz quedó \textit{exactamente} igual, pero sí quedaron similares. \\

Dado que es complicado visulizar diferencias entre vectores tridimensionales, analizaremos por un lado el eje $z$ y por el otro los ejes $x$ e $y$. En el siguiente gráfico tomamos para cada luz, los ejes $z$ y calculamos su diferencia. \\

{\centering
    \includegraphics[scale=0.7]{informe/imagenes/lucesEjezDiferencias.pdf} \\
}

Podemos observar que la la diferencia máxima es aproximadamente $0.1$ mientras que la diferencia mínima está cercana a cero (es exactamente $0.001821$ en la luz $1$). Creemos que si bien no es perfecto, es una diferencia aceptable. Dado que todos los vectores son unitarios (pues fueron normalizados) es lógico esperar diferencias entre los ejes x e y. A continuación puede verse el gráfico resultante. Las luces que se corresponden con las propias y las de la cátedra están señaladas con el mismo color. \\

Podemos observar que si bien las diferencias son notorias, todas se encuentran en el mismo rango en inclinación. No hay ninguna que haga cosas extrañas, como por ejemplo apuntar en sentido inverso. Creemos entonces que las diferencias no afectaron demasiado el resultado final.

{\centering
    \includegraphics[scale=0.8]{informe/imagenes/lucesCatedraProyeccionXY.pdf} \\
    % \captionof{figure}{Luces de la cátedra en la proyección x,y}
}
{\centering
    \includegraphics[scale=0.8]{informe/imagenes/lucesPropiasProyeccionXY.pdf} \\
    % \captionof{figure}{Luces propias en la proyección x,y}
}

% 2. Como afecta la calibracion del sistema en el resto de las etapas \\
\subsection{Normales}
Dado que las luces son utilizadas para el cálculo de las normales, queremos ver cómo nos afecta la calibración en esta etapa. Para eso, resolveremos el sistema de ecuaciones y guardaremos los vectores normales obtenidos. Como las mayores diferencias entre luces podían verse en los ejes $x, y$ estos ejes serán los que usaremos en la comparación. \\

En los gráficos que se encuentran a continuación, para cada píxel se grafica un vector que corresponde a la normal en ese punto. Como son \textit{miles} de normales, puede dar la impresión de que se grafican puntos pero no es así: son los vectores en forma de 'flechas'. \\

En todos los gráficos se utiliza exactamente la misma área de la imagen para que sean comparables. Para la resolución del sistema que calcula las normales se ultiliza eliminación gaussiana con pivoteo parcial para tratar de minimizar el error numérico. Para el primer experimento tomaremos nuestra 'mejor luz', la $1$ junto con las luces $4$ y $5$ que parecen ser las 'peores'. Esperábamos obtener malos resultados ya que dos luces no eran buenas, sin embargo los resultados fueron bastante buenos. \\

Las diferencias mas notorias se encuentran en el ojo y en la oreja del caballo. Creemos que tiene que ver porque son las áreas con más detalle pero menor iluminación. Sin embargo, no creemos que sea una diferencia demasiado significativa.

% {\centering
%     \includegraphics[scale=0.5]{informe/imagenes/normales/normalesCaballoLucesPropias145.pdf} \\
%     \includegraphics[scale=0.5]{informe/imagenes/normales/normalesCaballoLucesCatedra145.pdf}
% }


\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
    \centering
        \includegraphics[scale=0.5]{informe/imagenes/normales/normalesCaballoLucesCatedra145.pdf}
    \captionof{figure}{Nomales con luces catedra: 1, 4, 5}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \centering
        \includegraphics[scale=0.5]{informe/imagenes/normales/normalesCaballoLucesPropias145.pdf} \\
    \captionof{figure}{Normales con luces propias: 1, 4, 5}
\end{minipage}
\end{figure}

Aprovecharemos las siguientes imágenes podremos observar dos cosas. La primera es ver un poco más sobre las diferencias entre las luces de la cátedra y las calculadas por nosotros, y lo segundo es como cambian las normales cuando se toma un set de luces diferentes. \\

En el siguiente caso no tomamos la luz número $1$, que era la mejor que teníamos y en su lugar tomamos otra de las 'malas'. En este caso, consideraremos las luces $4, 5$ y $6$. Podemos observar que tanto las de la cátedra como las propias son demasiado oscuras, sin embargo la nuestra lo es mucho más. Los rasgos faciales dejaron de apreciarse. \\

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
    \centering
        \includegraphics[scale=0.5]{informe/imagenes/normales/normalesCaballoLucesCatedra456.pdf}
    \captionof{figure}{Nomales con luces catedra: 4, 5, 6}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \centering
        \includegraphics[scale=0.5]{informe/imagenes/normales/normalesCaballoLucesPropias456.pdf} \\
    \captionof{figure}{Normales con luces propias: 4, 5, 6}
\end{minipage}
\end{figure}

Al haber tomado otro set de luces, la forma general de la imagen no cambia demasiado, pero si hay bastante diferencia entre los rangos mas finos.

\subsection{Eliminacion gaussiana y Factorización LU}

Lo que queremos lograr con esta experimentación es comparar el resultado del cálculo sobre todos los pixeles usando el método de eliminación gaussiana vs factorizar la matriz y resolver los dos sistemas triangulados.
Para esto tomamos una matriz cualquiera de las generadas por 3 luces y resolvimos el sistema para N términos independientes distintos generados al azar. Esto simula el correr el algoritmo sobre una imagen de N pixeles.\\

Medimos únicamente el tiempo que tarda en resolver el sistema para cada píxel, dejando de lado todo cálculo extra por afuera de la medición (como por ejemplo la generacion aleatoria de los términos independientes). Luego sumamos cada uno de estos tiempos como la medición del experimento de tamaño N.
Hicimos 100 mediciones para cada cantidad de pixeles y luego dividimos por esa cantidad para sacar el promedio, de esta manera reducimos el ruido que pudo haberse visto durante la medición.

Hicimos varias mediciones variando la cantidad de pixeles y estos fueron los resultados:

{\centering
    \includegraphics[scale=0.7]{informe/imagenes/LUVSNOLU.PNG} \\
}

Como vemos ambos casos están en funcion lineal con la cantidad de pixeles, esto es gracias a que, aunque resolver un sistema sea cuadratico y el otro cubico, ambos estan resolviendo sistemas de 3x3 N veces, por lo tanto el tiempo que tarda en resolver un sistrema u otro se vuelve una constante que multiplica a N y por lo tanto tenemos ordenes lineales para los dos casos.\\

Es por esto que quisimos hacer otro experimento mas para comparar la diferencia entre el tiempo cuadratico de resolver el sistema ya factorizado y de resolver el sistema utilizando eliminacion gaussiana.
Para esto creamos instancias de matrices cada ves mas grandes y las duplicamos, a una le aplicamos eliminacion gaussiana y a la otra la factorizamos y luego resolvimos los dos sistemas (L y U). Es importante destacar que no tomamos el tiempo que se toma la factorizacion ya que este es un tiempo que se amortiza en la cantidad de operaciones y no era lo que queriamos comprobar.

El resultado fue el siguiente:

{\centering
    \includegraphics[scale=0.6]{informe/imagenes/LUVSNOLUDIM.PNG} \\
}

Dado que los tiempos para la medición de LU son muy pequeños, decidimos hacer otro gáafico con escala logarítmica para apreciar mejor la diferencia.


{\centering
    \includegraphics[scale=0.6]{informe/imagenes/LUVSNOLUDIMLOG.PNG} \\
}

Como podemos ver, efectivamente hay un crecimiento polinomial de ambos y la factorizacion LU crece en menor magnitud con el tamaño del sistema a resolver.


\subsection{Cholesky}

A continuación presentamos un análisis temporal de los métodos utilizados comparados con Cholesky. Para realizar estos análisis y poder hacer compararciones con la eliminació gaussiana y LU, generamos matrices que todos los métodos pueden resolver. Creamos de forma automatizada matrices simétricas definidas positivas, generando matrices $A$ con coeficientes aleatorios de 0 a 1 de tamaño $n$ y las multiplicamos por su traspuesta $A^{t}$. Luego les sumamos el valor $n$ en la diagonal garantizando que sean diagonal dominante y se las multiplica por un escalar para evitar valores muy cercanos al 0 que podrían ser inconvenientes para los métodos menos estables. \\

% En estos experimentos intentamos corroborar la complejidad asintótica de los métodos, generando matrices de dimensiones crecientes que resolvimos iterativamente por cada método, promediando el tiempo de ejecución para cada tamaño de matriz. En las mediciones no consideramos lo tiempos de creación de matrices y/o descomposición, sino que únicamente el tiempo de resolución. \\

Una de las ventajas mas importantes de las descomposiciones utilizadas es la de no tener que recalcular la matriz a resolver en cada iteración realizada si solo se cambia el término independiente. Para observar este comportamiento utilizamos matrices de dimensión $500*500$ y resolvimos el sistema $Ax = b$ varias veces midiendo el tiempo que se tomaba en resolver el mismo. \\

Lo esperado es que Gauss sea el método más lento en este tipo de pruebas, ya que debe re-triangular la matriz cada vez que quiere resolver para un nuevo $b$. En cambio, Cholesky y LU una vez que su descomposición es encontrada, solo deben realizarse despejes. Sobre el eje $x$ puede verse la \textit{cantidad acumulada} de términos independientes, y en el eje $y$ el tiempo en segundos. \\
% Por un lado simplemente medimos el tiempo total que tardaba cada iteracion de resolucion y por el otro sumamos los tiempos tomados en resolver todos los sistemas anteriores.
\hspace*{-2cm} \includegraphics[scale=0.55]{informe/imagenes/tytalus/DistintosBSuma-Dim500.pdf}

Podemos observar en el gráfico que se cumple lo esperado. Dado que la diferencia entre LU y Cholesky no es apreciable, veamos los mismos datos utilizados en el gráfico anterior pero esta vez sin incluir a Gauss.

\hspace*{-2cm} \includegraphics[scale=0.5]{informe/imagenes/tytalus/DistintosBSuma-soloCholeskyLU-Dim500.pdf}



% {\centering
%     \includegraphics[scale=0.5]{informe/imagenes/tytalus/DistintosB-Dim500.pdf}
% }

% {\centering
%     \includegraphics[scale=0.5]{informe/imagenes/tytalus/DistintosB-soloCholeskyLU-Dim500.pdf}
% }

